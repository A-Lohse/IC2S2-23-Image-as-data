{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRHWL0JgWvGC"
      },
      "source": [
        "Introduction to the whole re-train thing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvgcNnUq_Ilx",
        "outputId": "de55d9a9-94fd-4b55-dbe8-fd29650cd03f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary                    \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh7GJZLZUeOr"
      },
      "source": [
        "# Load models\n",
        "\n",
        "We will start by testing a pre-trained vgg16 model. You can read a bit about the pytorch implementation here:\n",
        "\n",
        "- https://pytorch.org/vision/master/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16\n",
        "\n",
        "\n",
        "And here is a schematic of the vgg16 architecture:\n",
        "- https://miro.medium.com/max/850/1*_Lg1i7wv1pLpzp2F4MLrvw.png\n",
        "\n",
        "Hopefully the architecture looks somewhat familiar and you can recognize the \"encoder\" form. The reason it is denoted *16* is that this vgg implementation have 16 trainable layers (the blue and green blocks in the architecture schematics above). \n",
        "\n",
        "The point for this exercise is to try different pre-trained networks out. That also means that most of the code below needs no tweaking. When you have changed what little needs to be changed for the vgg16 model to run, and you have noted its performance, I suggest you try to implement these models:\n",
        "\n",
        "- resnet18\n",
        "- mobilenet_v3_small\n",
        "- shufflenet_v2_x0_5\n",
        "- squeezenet1_0\n",
        "\n",
        "You can read about these - and meny more under *classification* at:\n",
        "- https://pytorch.org/vision/master/models.html#classification\n",
        "\n",
        "Note that some of these networks are quite a bit more complicated - and larger - than e.g. the vgg16. This should not discourage you. For now, you do not have to understand the more complicated architectures. You just need to be able to re-train the networks. Of course, you would always want to read the article(s) presenting the networks if you where to use them for actual research, but since we are just playing around the pytorch documentation will do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXEvDuOWUSO-"
      },
      "outputs": [],
      "source": [
        "# Load the vgg16 model-builder\n",
        "\n",
        "from torchvision.models import vgg16_bn # bn is simply the version with batch normalization\n",
        "\n",
        "# for TA's\n",
        "# from torchvision.models import resnet18, vgg11_bn, mobilenet_v3_small, squeezenet1_0, shufflenet_v2_x0_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrgEA1StWdg9"
      },
      "outputs": [],
      "source": [
        "# lest also define our classes here. We will need this in a moment.\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "num_classes = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "88c35af0ed9f4d9a95cb1ae355cf488f",
            "52b02e9e1aaf46aebc8ed9b71303c6ef",
            "bdb4cd04796a4f6eb609d13a760c3178",
            "144d08449f594cd5b62a88adba991e49",
            "6b1a6db11a5547e3a53b49da80fa88e4",
            "5351e7fd31564ac8bc1187ebd574032b",
            "59962811fa774cab8e33d69b7fdae9fe",
            "0d6c9b56426144a8828614e2fab873ed",
            "dfbf4b6a25484d74b6d991d65d2b8a25",
            "98328df6fa6b4d06963ab5858b0c6e2c",
            "bf608b1c5a9c4882b59721b9dd32b40b"
          ]
        },
        "id": "4rkOyLFncFF7",
        "outputId": "63c03e9b-22a6-41f9-938b-d4d5bd15ee06"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88c35af0ed9f4d9a95cb1ae355cf488f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#instanciate a pretrained vgg16_bn model (and send it to the gpu)\n",
        "\n",
        "model = vgg16_bn(pretrained = True).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbkYkgpDjnPT"
      },
      "source": [
        "The fact that you have instantiated the model as \"pre-trained\" means that all the weights in all the filters/kernels are already trained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdt7LMOTjmLk",
        "outputId": "35f15ff5-bd6d-4753-d3d7-02e032ef890e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# print the model architectures\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndhNqtbrcXUD"
      },
      "source": [
        "You can also use \n",
        "`summary(model, (3,224,244))`\n",
        "or\n",
        "`for k in model._modules.keys():\n",
        "  print(k)`\n",
        "To survey the model if you want to.\n",
        "\n",
        "As you might see this model is somewhat similar to what you created last week. The most central part of this print out (right now) is the last layer. As you can see it right now, the output of the model is 1000 different classes. If we were using a dataset with the same 1000 classes we could just go ahead. But We only have 10 in the our dataset CIFAR10. So we need to change this last layer.\n",
        "\n",
        "If you look at the very bottom of the printout above you should see something like:\n",
        "\n",
        "`(classifier): Sequential(`   \n",
        "`...`  \n",
        "`(6): Linear(in_features=4096 , out_features=1000, bias=True)`  \n",
        "`)`\n",
        "\n",
        "So, we want to change the 6th object in the classifier *block*. Specifically, you want to insert a new linear layer here. It should take the same number of in_features but the number of out-features should correspond the number of classes in our task. \n",
        "\n",
        "Note that this specific for Alexnet (and a couple different models). Other models have different architecture and the last layer that needs changing might be called something else, be placed differently, or might not even be Linear (remember that you can do convolutions all the way down). So you will need to check each model. If you get stuck here, this page might help you out a bit. Go to **Initialize and Reshape the Networks**\n",
        ":\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "\n",
        "Also note: if you do not want to hard-code or remember the number of input features use:\n",
        "\n",
        "`model.classifier[6].in_features`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGtAqHM3cFCd"
      },
      "outputs": [],
      "source": [
        "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53bZASBjsMA2"
      },
      "source": [
        "If you now print the last layer, you should see that the appropriate amount of out_features: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBOyZ0bdsJ7-",
        "outputId": "9b7b29b5-0451-4d00-e895-a134fb445c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=4096, out_features=10, bias=True)\n"
          ]
        }
      ],
      "source": [
        "print(model.classifier[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOCKFGu0sZm_"
      },
      "source": [
        "\n",
        "Note that this layer is now completely new. There is nothing pre-trained in it. Only \"randomly\" initialized weights. These needs to be trained. But! All the other layers is still pre-trained.\n",
        "\n",
        "Now, just as we changed the output of our model, we must also change our input. I.e. the images (not the model this time). All pre-trained models expect a input of a specific size (and dimension). For Alexnet you can see it here (bottom page):\n",
        "\n",
        "https://pytorch.org/vision/master/models/generated/torchvision.models.alexnet.html#torchvision.models.alexnet\n",
        "\n",
        "What you should focus on (rigth now) is that the network expects a image of 224x224 pixels. I also expects the image to be normalized between [-1, 1]. A couple of things are worth noting here:\n",
        "\n",
        "- Since these ar CNN they can take images of many varying sizes, but the result will suffer (a lot sometimes) is you do no resize you input images accordingly.\n",
        "- Whether you resize to 256 and then center crop to 224 (as stated in the manual) is less important. You can also resize to 256 and random crop for better data augmentation (doing training only of course).\n",
        "- According to the manual you should first re-scale the image to [0,1] then normalize using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. In practice normalizing the CIFAR10 images with mean=[0.5, 0.5, 0.5] and std=[0.5, 0.5, 0.5] achieves something very similar: normalization between [-1, 1].\n",
        "\n",
        "I have put a (working) suggestion below, but feel free to experiment with different transformations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM7WT0Pyte_Q"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.RandomCrop(224), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(10), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "transform_test = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.CenterCrop(224), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym3h4GoF1avY"
      },
      "source": [
        "# ALTERNATIVE PRE-TRAINED MODEL IMPLEMENTATIONS FOR TAs:\n",
        "what you should note is the difference between how I change the last layer give different models. Especially Squeeznet differs as its layer is a conv2d layer and not a dense linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XalNwc3FoPj"
      },
      "outputs": [],
      "source": [
        "# # RESNET18 - works\n",
        "\n",
        "# #https://pytorch.org/vision/master/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18\n",
        "\n",
        "# from torchvision.models import resnet18\n",
        "\n",
        "# model = resnet18(pretrained = True).to(device) # old API # 79% after 4 epochs\n",
        "# model.fc = nn.Linear(model.fc.in_features, num_classes).to(device) # model.fc.in_features = 2048\n",
        "\n",
        "# transform_train = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.RandomCrop(224), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(10), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform_test = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.CenterCrop(224), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WIFAJFYFpKA"
      },
      "outputs": [],
      "source": [
        "# # MOBILENET_V3_SMALL - works\n",
        "# #https://pytorch.org/vision/master/models/generated/torchvision.models.mobilenet_v3_small.html#torchvision.models.mobilenet_v3_small\n",
        "\n",
        "# from torchvision.models import mobilenet_v3_small\n",
        "\n",
        "# model = mobilenet_v3_small(pretrained = True).to(device)  # old API # 82% after 4 epochs\n",
        "# model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes).to(device) # model.fc.in_features = 2048\n",
        "\n",
        "# transform_train = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.RandomCrop(224), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(10), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform_test = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.CenterCrop(224), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjlrdLn8FpnA"
      },
      "outputs": [],
      "source": [
        "# # SHUFFLENET_v2_x0_5\n",
        "# # https://pytorch.org/vision/master/models/generated/torchvision.models.shufflenet_v2_x0_5.html#torchvision.models.shufflenet_v2_x0_5\n",
        "\n",
        "# from torchvision.models import shufflenet_v2_x0_5\n",
        "\n",
        "# model = shufflenet_v2_x0_5(pretrained = True).to(device) # 81 after 4 epochs\n",
        "# model.fc = nn.Linear(model.fc.in_features, num_classes).to(device) # model.fc.in_features = 2048\n",
        "\n",
        "# transform_train = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.RandomCrop(224), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(10), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform_test = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.CenterCrop(224), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd_8S2JSFptW"
      },
      "outputs": [],
      "source": [
        "# # SQUEEZENET1_0\n",
        "# # https://pytorch.org/vision/master/models/generated/torchvision.models.squeezenet1_0.html#torchvision.models.squeezenet1_0\n",
        "\n",
        "# from torchvision.models import squeezenet1_0\n",
        "\n",
        "# model = squeezenet1_0(pretrained = True).to(device)  # old API # 52 after 4 epochs\n",
        "# model.classifier[1] = nn.Conv2d(in_channels = model.classifier[1].in_channels, out_channels = num_classes, kernel_size = (1,1), stride = (1,1)).to(device) # model.fc.in_features = 2048\n",
        "# model.num_classes = num_classes # squeezenet1_1 needs this\n",
        "\n",
        "# transform_train = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.RandomCrop(224), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(10), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "# transform_test = transforms.Compose([transforms.ToTensor(), transforms.Resize(256), transforms.CenterCrop(224), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P2Ysu0-xQNy"
      },
      "source": [
        "And we can now create our data loaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "a3ee4fec39e6485981cb272f3bef989c",
            "b70320afde854039863f005f220f4086",
            "cbb2bae0b80f4a968cde8939d24bff6f",
            "4db24489f6af4dfe8222d80a9bef1d1e",
            "cbf1f13ab3e943d5be820ef4dacf460e",
            "c350a4113f484cc7b07db8beefb6a530",
            "50253f048ff3458095adc313d30301d0",
            "ef519c25a31748898ecb6ef519528ae0",
            "b4c566a7f9bd4cb69bc5f371fb574f1c",
            "1e614f2593124db09e2a669a33d25bca",
            "fc30a8c8bded4cbfbe4a52612cd812b4"
          ]
        },
        "id": "y-58-vOktf-d",
        "outputId": "2086e8f3-ffda-455b-a5e0-963fbdd9a95a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3ee4fec39e6485981cb272f3bef989c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "number of samplestorch.Size([4, 3, 224, 224]) (batch size, color channel, width, hieght)\n",
            "number of labels torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "os.makedirs('data', exist_ok = True)\n",
        "os.makedirs('models', exist_ok = True)\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter=iter(trainloader)\n",
        "images,labels=next(dataiter)\n",
        "\n",
        "print(f\"number of samples{images.shape} (batch size, color channel, width, hieght)\")\n",
        "print(f\"number of labels {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT1eBy6kxcfO"
      },
      "source": [
        "Note from the print out above that the images are now 3x224x224 and not 3x32x32. (the first dim = 4 is the batch dimension)\n",
        "\n",
        "Before we go further, I just show a couple of different ways you can examine \n",
        "\n",
        "your network architecture - as supplements to just \"print(model)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4sbUMkjtf5V",
        "outputId": "734d94f6-ef46-4dfb-d30f-64ebdfea4e30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 244]           1,792\n",
            "       BatchNorm2d-2         [-1, 64, 224, 244]             128\n",
            "              ReLU-3         [-1, 64, 224, 244]               0\n",
            "            Conv2d-4         [-1, 64, 224, 244]          36,928\n",
            "       BatchNorm2d-5         [-1, 64, 224, 244]             128\n",
            "              ReLU-6         [-1, 64, 224, 244]               0\n",
            "         MaxPool2d-7         [-1, 64, 112, 122]               0\n",
            "            Conv2d-8        [-1, 128, 112, 122]          73,856\n",
            "       BatchNorm2d-9        [-1, 128, 112, 122]             256\n",
            "             ReLU-10        [-1, 128, 112, 122]               0\n",
            "           Conv2d-11        [-1, 128, 112, 122]         147,584\n",
            "      BatchNorm2d-12        [-1, 128, 112, 122]             256\n",
            "             ReLU-13        [-1, 128, 112, 122]               0\n",
            "        MaxPool2d-14          [-1, 128, 56, 61]               0\n",
            "           Conv2d-15          [-1, 256, 56, 61]         295,168\n",
            "      BatchNorm2d-16          [-1, 256, 56, 61]             512\n",
            "             ReLU-17          [-1, 256, 56, 61]               0\n",
            "           Conv2d-18          [-1, 256, 56, 61]         590,080\n",
            "      BatchNorm2d-19          [-1, 256, 56, 61]             512\n",
            "             ReLU-20          [-1, 256, 56, 61]               0\n",
            "           Conv2d-21          [-1, 256, 56, 61]         590,080\n",
            "      BatchNorm2d-22          [-1, 256, 56, 61]             512\n",
            "             ReLU-23          [-1, 256, 56, 61]               0\n",
            "        MaxPool2d-24          [-1, 256, 28, 30]               0\n",
            "           Conv2d-25          [-1, 512, 28, 30]       1,180,160\n",
            "      BatchNorm2d-26          [-1, 512, 28, 30]           1,024\n",
            "             ReLU-27          [-1, 512, 28, 30]               0\n",
            "           Conv2d-28          [-1, 512, 28, 30]       2,359,808\n",
            "      BatchNorm2d-29          [-1, 512, 28, 30]           1,024\n",
            "             ReLU-30          [-1, 512, 28, 30]               0\n",
            "           Conv2d-31          [-1, 512, 28, 30]       2,359,808\n",
            "      BatchNorm2d-32          [-1, 512, 28, 30]           1,024\n",
            "             ReLU-33          [-1, 512, 28, 30]               0\n",
            "        MaxPool2d-34          [-1, 512, 14, 15]               0\n",
            "           Conv2d-35          [-1, 512, 14, 15]       2,359,808\n",
            "      BatchNorm2d-36          [-1, 512, 14, 15]           1,024\n",
            "             ReLU-37          [-1, 512, 14, 15]               0\n",
            "           Conv2d-38          [-1, 512, 14, 15]       2,359,808\n",
            "      BatchNorm2d-39          [-1, 512, 14, 15]           1,024\n",
            "             ReLU-40          [-1, 512, 14, 15]               0\n",
            "           Conv2d-41          [-1, 512, 14, 15]       2,359,808\n",
            "      BatchNorm2d-42          [-1, 512, 14, 15]           1,024\n",
            "             ReLU-43          [-1, 512, 14, 15]               0\n",
            "        MaxPool2d-44            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-45            [-1, 512, 7, 7]               0\n",
            "           Linear-46                 [-1, 4096]     102,764,544\n",
            "             ReLU-47                 [-1, 4096]               0\n",
            "          Dropout-48                 [-1, 4096]               0\n",
            "           Linear-49                 [-1, 4096]      16,781,312\n",
            "             ReLU-50                 [-1, 4096]               0\n",
            "          Dropout-51                 [-1, 4096]               0\n",
            "           Linear-52                   [-1, 10]          40,970\n",
            "================================================================\n",
            "Total params: 134,309,962\n",
            "Trainable params: 134,309,962\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.63\n",
            "Forward/backward pass size (MB): 350.19\n",
            "Params size (MB): 512.35\n",
            "Estimated Total Size (MB): 863.16\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "summary(model, (3,224,244))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBYHRwQ9tf0z",
        "outputId": "cf2e285a-5384-4786-8b8d-f3d0af550216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features\n",
            "avgpool\n",
            "classifier\n"
          ]
        }
      ],
      "source": [
        "for k in model._modules.keys():\n",
        "  print(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XlWBJi7ydLs"
      },
      "source": [
        "We can also check which parameters are trainable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5P6xYbvycHz",
        "outputId": "d8ea82b7-e998-464a-d91a-9f138253b583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t features.0.weight\n",
            "\t features.0.bias\n",
            "\t features.1.weight\n",
            "\t features.1.bias\n",
            "\t features.3.weight\n",
            "\t features.3.bias\n",
            "\t features.4.weight\n",
            "\t features.4.bias\n",
            "\t features.7.weight\n",
            "\t features.7.bias\n",
            "\t features.8.weight\n",
            "\t features.8.bias\n",
            "\t features.10.weight\n",
            "\t features.10.bias\n",
            "\t features.11.weight\n",
            "\t features.11.bias\n",
            "\t features.14.weight\n",
            "\t features.14.bias\n",
            "\t features.15.weight\n",
            "\t features.15.bias\n",
            "\t features.17.weight\n",
            "\t features.17.bias\n",
            "\t features.18.weight\n",
            "\t features.18.bias\n",
            "\t features.20.weight\n",
            "\t features.20.bias\n",
            "\t features.21.weight\n",
            "\t features.21.bias\n",
            "\t features.24.weight\n",
            "\t features.24.bias\n",
            "\t features.25.weight\n",
            "\t features.25.bias\n",
            "\t features.27.weight\n",
            "\t features.27.bias\n",
            "\t features.28.weight\n",
            "\t features.28.bias\n",
            "\t features.30.weight\n",
            "\t features.30.bias\n",
            "\t features.31.weight\n",
            "\t features.31.bias\n",
            "\t features.34.weight\n",
            "\t features.34.bias\n",
            "\t features.35.weight\n",
            "\t features.35.bias\n",
            "\t features.37.weight\n",
            "\t features.37.bias\n",
            "\t features.38.weight\n",
            "\t features.38.bias\n",
            "\t features.40.weight\n",
            "\t features.40.bias\n",
            "\t features.41.weight\n",
            "\t features.41.bias\n",
            "\t classifier.0.weight\n",
            "\t classifier.0.bias\n",
            "\t classifier.3.weight\n",
            "\t classifier.3.bias\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n"
          ]
        }
      ],
      "source": [
        "print(\"Params to learn:\")\n",
        "\n",
        "for name,param in model.named_parameters():\n",
        "  if param.requires_grad == True:\n",
        "    print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdONpHbExjlE"
      },
      "source": [
        "also we do not need to re-train all layers. If we want to, we could choose to only retrain the last layer (or layers). This is called feature extraction. That is, we are not just using a pre-trained network. We are using the specific features learned in that in network without changing them (or most of them).\n",
        "\n",
        "In the code below I freeze all parameters apart from the bias and weight in the last layer. Again note that this code is not universal. It works for vgg16 and a couple others, but networks with different architectures might need different handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRpicfr-xiRE"
      },
      "outputs": [],
      "source": [
        "for param in list(model.parameters())[:-2]: # -2 save the last two parameters in Alexnet.\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzJbdD1Nz4ND"
      },
      "source": [
        "If we now print the trainable parameters we see that only the last layer (the weights and the biases here) are trainable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkbGYwGotfrP",
        "outputId": "d3489108-bb98-4729-96c8-5c6e41ecd834"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n"
          ]
        }
      ],
      "source": [
        "print(\"Params to learn:\")\n",
        "\n",
        "for name,param in model.named_parameters():\n",
        "  if param.requires_grad == True:\n",
        "    print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLu3cw1iz4m3"
      },
      "source": [
        "And you could also choose to keep more bare parameters learnable, like all three layers in the dense classifier for vgg16 (-6 instead of -2). For now, however, we want to retrain all parameters (feel free to go back and experiment later)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN4eZG94tfmB"
      },
      "outputs": [],
      "source": [
        "# train all parameters\n",
        "for param in list(model.parameters()):\n",
        "  param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q850b3wutgjt"
      },
      "source": [
        "\n",
        "And, we can check that all the parameters are now trainable again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuQDWVNscE-v",
        "outputId": "a1422103-8cdc-43ac-f47c-3cffd8572cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t features.0.weight\n",
            "\t features.0.bias\n",
            "\t features.1.weight\n",
            "\t features.1.bias\n",
            "\t features.3.weight\n",
            "\t features.3.bias\n",
            "\t features.4.weight\n",
            "\t features.4.bias\n",
            "\t features.7.weight\n",
            "\t features.7.bias\n",
            "\t features.8.weight\n",
            "\t features.8.bias\n",
            "\t features.10.weight\n",
            "\t features.10.bias\n",
            "\t features.11.weight\n",
            "\t features.11.bias\n",
            "\t features.14.weight\n",
            "\t features.14.bias\n",
            "\t features.15.weight\n",
            "\t features.15.bias\n",
            "\t features.17.weight\n",
            "\t features.17.bias\n",
            "\t features.18.weight\n",
            "\t features.18.bias\n",
            "\t features.20.weight\n",
            "\t features.20.bias\n",
            "\t features.21.weight\n",
            "\t features.21.bias\n",
            "\t features.24.weight\n",
            "\t features.24.bias\n",
            "\t features.25.weight\n",
            "\t features.25.bias\n",
            "\t features.27.weight\n",
            "\t features.27.bias\n",
            "\t features.28.weight\n",
            "\t features.28.bias\n",
            "\t features.30.weight\n",
            "\t features.30.bias\n",
            "\t features.31.weight\n",
            "\t features.31.bias\n",
            "\t features.34.weight\n",
            "\t features.34.bias\n",
            "\t features.35.weight\n",
            "\t features.35.bias\n",
            "\t features.37.weight\n",
            "\t features.37.bias\n",
            "\t features.38.weight\n",
            "\t features.38.bias\n",
            "\t features.40.weight\n",
            "\t features.40.bias\n",
            "\t features.41.weight\n",
            "\t features.41.bias\n",
            "\t classifier.0.weight\n",
            "\t classifier.0.bias\n",
            "\t classifier.3.weight\n",
            "\t classifier.3.bias\n",
            "\t classifier.6.weight\n",
            "\t classifier.6.bias\n"
          ]
        }
      ],
      "source": [
        "print(\"Params to learn:\")\n",
        "\n",
        "for name,param in model.named_parameters():\n",
        "  if param.requires_grad == True:\n",
        "    print(\"\\t\",name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjawVQG029-2"
      },
      "source": [
        "\n",
        "And, we just go fine-tune/re-train our pre-trained model no bigger here. Feel free the experiment with different hyper parameters and optimizer.\n",
        "\n",
        "# NOTE THAT THESE ARE LARGE MODELS. THEY WILL TAKE TIME TO TRAIN EVEN ON A GPU.\n",
        "\n",
        "So go read the curriculum or some such in the mean time..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7_mATioHqrh"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "momentum = 0.9\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx71RL2UH5Zn"
      },
      "outputs": [],
      "source": [
        "# set the model to train mode\n",
        "model.train()\n",
        "\n",
        "n_epochs = 4\n",
        "\n",
        "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    history_loss = []\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # loss + backward\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        \n",
        "        # optimize\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        history_loss.append(running_loss / len(trainset))\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so1k2EXyGLtl"
      },
      "source": [
        "Lets plot some test images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnVeMskNITIq"
      },
      "outputs": [],
      "source": [
        "dataiter_test = iter(testloader)\n",
        "images_test, labels_test = dataiter_test.next()\n",
        "images_test = images_test.to(device)\n",
        "labels_test = labels_test.to(device)\n",
        "\n",
        "outputs = model(images_test)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "def show_predictions(images = images_test, labels = labels_test, classes = classes, predicted = predicted, n = 4):\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    \n",
        "    for i in range(n):\n",
        "      plt.subplot(1,n,i+1)\n",
        "    \n",
        "      img = images[i] / 2 + 0.5 # one image from batch and unnormalize\n",
        "      npimg = img.cpu().numpy() # from tensor to numpy\n",
        "      plt.imshow(np.transpose(npimg, (1, 2, 0))) # from shape (3,32,32) -> (32,32,3) bc imshow...\n",
        "      plt.title(f'true class: {classes[labels[i]]}\\npredicted class: {classes[predicted[i]]}')\n",
        "    plt.show()\n",
        "\n",
        "# show images\n",
        "show_predictions(images = images_test, labels = labels_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7sO3PFBs2wl"
      },
      "source": [
        "Find the overall accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM4d9wr6IVOO"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "\n",
        "        # if you run on gpu\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HLke7cXs5im"
      },
      "source": [
        "And the class specific accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwm2FcDJIXvz"
      },
      "outputs": [],
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "\n",
        "model.eval() # should not be neccesary ince you fd owith no grad\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "          # if you run on gpu\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "766W7TGeiSHS"
      },
      "source": [
        "Now go back and try a different model, e.g. mobilenet_v3_small. See if you can beat vgg16_bn.\n",
        "\n",
        "And now couple of concluding remarks:\n",
        "\n",
        "1. The next real challenge is using your own data. First you need to label it. This takes time but given that you now know a bit about using pre-trained networks, you might only need to label 100-1000 images to get okay results.\n",
        "\n",
        "2. It can be a hassle creating you own data loader of custom data, but it is 100% doable. It might just require some trial and error.\n",
        "\n",
        "3. We have only really done image classification here. Chances are that if you are doing anything more serious you might want to do object detection or segmentation. Don't worry: the models are a bit more complicated but they exist and you can use pre-trained models here as well.\n",
        "\n",
        "4. Also, for custom annotations for object detection and segmentation tools such as LabelImg can help: https://github.com/tzutalin/labelImg.\n",
        "\n",
        "5. The Pytorch API for pre-trained models is going to change a bit in the future. Nothing major, but you will be able to choose between different pre-trained weights for each model. That is super nice, but it also means that the `pretrained = True` argument is going to be deprecated. See https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/ for more.\n",
        "\n",
        "6. Lastly, for this for this (and the previous) exercise we have imported models straight from pytorch and used native pytorch for everything. This is works very well, but if you going to use pre-trained models and want something a bit more powerful look into detectron2. This is especially important if you want to do more complicated stuff like object detection and semantic. Detectron2 is kind of a API wrapper around a lot of pre-defined and pre-trained models all implemented in pytorch. It is super powerful and it works very similar to what you just did. Indeed it is actually a bit more intuitive here and there when you get to know it. This links might prove useful. \n",
        "\n",
        "\n",
        "https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/   \n",
        "https://detectron2.readthedocs.io/en/latest/   \n",
        "https://detectron2.readthedocs.io/en/latest/tutorials/getting_started.html   \n",
        "https://github.com/facebookresearch/detectron2   \n",
        "https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ASDS2022_2_solutions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d6c9b56426144a8828614e2fab873ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "144d08449f594cd5b62a88adba991e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98328df6fa6b4d06963ab5858b0c6e2c",
            "placeholder": "​",
            "style": "IPY_MODEL_bf608b1c5a9c4882b59721b9dd32b40b",
            "value": " 528M/528M [00:05&lt;00:00, 108MB/s]"
          }
        },
        "1e614f2593124db09e2a669a33d25bca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db24489f6af4dfe8222d80a9bef1d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e614f2593124db09e2a669a33d25bca",
            "placeholder": "​",
            "style": "IPY_MODEL_fc30a8c8bded4cbfbe4a52612cd812b4",
            "value": " 170499072/? [00:04&lt;00:00, 55480687.96it/s]"
          }
        },
        "50253f048ff3458095adc313d30301d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52b02e9e1aaf46aebc8ed9b71303c6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5351e7fd31564ac8bc1187ebd574032b",
            "placeholder": "​",
            "style": "IPY_MODEL_59962811fa774cab8e33d69b7fdae9fe",
            "value": "100%"
          }
        },
        "5351e7fd31564ac8bc1187ebd574032b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59962811fa774cab8e33d69b7fdae9fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b1a6db11a5547e3a53b49da80fa88e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c35af0ed9f4d9a95cb1ae355cf488f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52b02e9e1aaf46aebc8ed9b71303c6ef",
              "IPY_MODEL_bdb4cd04796a4f6eb609d13a760c3178",
              "IPY_MODEL_144d08449f594cd5b62a88adba991e49"
            ],
            "layout": "IPY_MODEL_6b1a6db11a5547e3a53b49da80fa88e4"
          }
        },
        "98328df6fa6b4d06963ab5858b0c6e2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ee4fec39e6485981cb272f3bef989c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b70320afde854039863f005f220f4086",
              "IPY_MODEL_cbb2bae0b80f4a968cde8939d24bff6f",
              "IPY_MODEL_4db24489f6af4dfe8222d80a9bef1d1e"
            ],
            "layout": "IPY_MODEL_cbf1f13ab3e943d5be820ef4dacf460e"
          }
        },
        "b4c566a7f9bd4cb69bc5f371fb574f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b70320afde854039863f005f220f4086": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c350a4113f484cc7b07db8beefb6a530",
            "placeholder": "​",
            "style": "IPY_MODEL_50253f048ff3458095adc313d30301d0",
            "value": ""
          }
        },
        "bdb4cd04796a4f6eb609d13a760c3178": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d6c9b56426144a8828614e2fab873ed",
            "max": 553507836,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfbf4b6a25484d74b6d991d65d2b8a25",
            "value": 553507836
          }
        },
        "bf608b1c5a9c4882b59721b9dd32b40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c350a4113f484cc7b07db8beefb6a530": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbb2bae0b80f4a968cde8939d24bff6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef519c25a31748898ecb6ef519528ae0",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4c566a7f9bd4cb69bc5f371fb574f1c",
            "value": 170498071
          }
        },
        "cbf1f13ab3e943d5be820ef4dacf460e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfbf4b6a25484d74b6d991d65d2b8a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef519c25a31748898ecb6ef519528ae0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc30a8c8bded4cbfbe4a52612cd812b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
